{"google":"","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Sabre","body":"### Abstract\r\nSabre is an in-memory distributed graph processing system designed for graphs which are big enough to warrant distributed computation but small enough to fit into memory (on the lowest memory machine).\r\n\r\n### Introduction\r\nSabre is an in-memory distributed graph processing system for trivially parallelizable algorithms. Take for example all pairs shortest path (APSP) in an undirected, unweighted graph. A simple algorithm that comes to mind is conducting an exhaustive breadth-first search on every node of the graph. Multithreading this code is fairly straightforward - you can have each thread process one node at a time (each thread gets a node and conducts a full BFS from that node) and display the results as it finishes. However, distributing this code across several machines is a bit more complicated. Ideally, we'd like the programmer to just focus on the algorithm at hand in a local context (see bullet point below) and have the system handle the distribution. Once the algorithm is written, the programmer specifies which machines to distribute over, and the system takes care of the rest. The goal of Sabre is to make this become a reality.\r\n\r\nMost distributed graph processing systems are designed to handle complex algorithms and make no assumptions of whether or not the graph fits into memory or not. Therefore, such systems trade simplicity for extensibility. However, sometimes we want to just run a simple algorithm on a graph that is large enough to warrant distributed computing, but small enough to fit into memory (this of course varies as a function of the graph size and the memory of the machine(s) in question). Sabre fills this niche.\r\n\r\nSabre is written in Scala 2.10.0-RC2 and Akka 2.1.0-RC2 and is designed to follow the philosophies shared by both - clean implementation with scalability in mind.\r\n\r\n* For the APSP problem, the algorithm in a local context would be a function that takes as input a graph G = (V, E) and a node u in V and returns a mapping for all nodes v in V - {u} to the shortest path between u and v.\r\n\r\n### Assumptions Made\r\n* Algorithms can be broken down into several sub-tasks that are independent of one another\r\n* The graph interface provided assumes undirected and unweighted graphs.\r\n\r\n### Functional Requirements\r\n* Easy to deploy - can be deployed on any set of machines that are able to communicate with each other (no firewall)\r\n* Tolerate computers with a let it crash mentality (no attempt will be made to revive the Worker, instead the work will be given to others)\r\n* Basic load balancing (first to request work will be given the work)\r\n* Control maximum number of threads to be used\r\n\r\n### Architecture\r\n* Master-Worker (client-server) - Master contains a queue of work, workers take work from master in a first-ask-first-receive basis\r\n* Master is deployed implicitly by client (client need only call Sabre.execute() to start up the system)\r\n* Workers register with Master and immediately start working\r\n\r\n### Experiments\r\nAll algorithms should only need be expressed in a local context. How the algorithm breaks down and how fine of a granularity work is broken down is up to the user. For example to simply print out the list of degrees in a graph, we can define the algorithm to take in a single node ID and return a single integer value representing the degree. However, we can just as easily define an algorithm to take a collection of nodes and return a `Map[Int, Int]` mapping the ID of the node to the degree of that node.\r\n\r\nExample algorithms include:\r\n\r\n* List of degrees of graph\r\n* All pairs shortest paths in unweighted, undirected graph using exhaustive BFS\r\n\r\n#### Parallel Efficiency\r\nBelow are plots describing the scalability of the system in terms of parallel efficiency. Tests were done in a cluster environment with NFS. Machines use Quad Core Xeon processors clocked @ 2.4 GHz with 24 GB RAM. We take a look at the parallel efficiency ratio, defined as (time taken for 1 worker) / (time taken for all workers * # of workers).\r\n\r\nFirst we show the parallel efficiency (both local and remote) of an exhaustive BFS on a complete graph with 200 nodes.\r\n\r\n![Local BFS Efficiency](http://cs.ucsb.edu/~adelbert_chang/290/localbfs.png)\r\n![Remote BFS Efficiency](http://cs.ucsb.edu/~adelbert_chang/290/remotebfs.png)\r\n\r\nNotice that the trend is consistent between local and remote - this is due to the fact that conducting an exhaustive BFS is significantly more complex than serialization. As such, the impact of serialization is minimized.\r\n\r\nYou can also see that it scales to multiple cores fairly well - with one Worker, it takes roughly 17 seconds to finish the work. With four workers, it takes about 4 seconds. That's a parallel efficiency ratio of almost 1!\r\n\r\nWe now look at the parallel efficiency of simply retrieving the degree of all nodes in the graph. Note that this is a very trivial operation, and the time taken to perform this operation is much smaller than an exhaustive BFS, and is much more comparable to the time taken for serialization. The graph used is a graph modeled after the structure of online social network, consisting of about 6000 nodes.\r\n\r\n![Local Degree Efficiency](http://cs.ucsb.edu/~adelbert_chang/290/localdeg.png)\r\n![Remote Degree Efficiency](http://cs.ucsb.edu/~adelbert_chang/290/remotedeg.png)\r\n\r\nBecause serialization is now a bigger factor in the computation, the scalability becomes much more limited. Where before it took 18 seconds to finish work with one Worker, it still takes about 9 seconds to finish with four workers. That's a parallel efficiency ratio of 0.5.\r\n\r\n### Possible Features\r\n* Arbitrary graph structures (directed, weighted, etc.)\r\n* Ability to introduce a \"reduce\" phase to the results (e.g. for average degree, follow the \"list of degrees\" example stated above, then take the average and display that instead)\r\n* Display task metadata (e.g. computation time) at the end of the computation\r\n\r\n### Slides\r\n[link](http://cs.ucsb.edu/~adelbert_chang/290/Sabre290Slides.pdf)","tagline":"In-memory distributed graph processing of trivially parallelizable graph algorithms."}